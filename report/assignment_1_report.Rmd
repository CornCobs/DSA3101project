---
title: "Group Assignment 1 Report"
author: "Group 2: Cai Anqi, Chan Yu Hang, Chin Synn Khee Joash, Chua Cheng Ling, Chua Hua Ren, Clarence Ong"
output: html_document
---

```{r message=F, warning=F, echo=F}
library(tidyverse)
library(readxl)
library(knitr)
library(lubridate)
library(reticulate)

cat_info <- read_csv("../data/DSA3101_Hackathon_Categories_Information.csv")
data <- read_csv("../data/DSA3101_Hackathon_Data.csv")
panel_demo <- read_excel("../data/DSA3101_Hackathon_Panelists_Demographics.xlsx")

panel_demo <- panel_demo %>% 
  mutate(Income=case_when(Income=="Income < 1500" ~ "[0, 1500)",
                        Income=="Income >5000" ~ "[5000, )",
                        Income=="Income 1500 - 1999" ~ "[1500, 2000)",
                        Income=="Income 2000 - 2999" ~ "[2000, 3000)",
                        Income=="Income 3000 - 3999" ~ "[3000, 4000)",
                        Income=="Income 4000 - 4999" ~ "[4000, 5000)"),
         Race=case_when(str_detect(Ethnicity, "Malay")~"Malay",
                             str_detect(Ethnicity, "Chinese")~"Chinese",
                             str_detect(Ethnicity, "Others")~"Others"))

# preprocessing code
# df <- data %>% left_join(cat_info, by="Category") %>% 
#   left_join(panel_demo, by=c("Panel ID" = "ID")) %>% 
#   mutate(Race=case_when(str_detect(Ethnicity, "Malay")~"Malay",
#                              str_detect(Ethnicity, "Chinese")~"Chinese",
#                              str_detect(Ethnicity, "Others")~"Others"),
#        Income=case_when(Income=="Income < 1500" ~ "[0, 1500)",
#                         Income=="Income >5000" ~ "[5000, )",
#                         Income=="Income 1500 - 1999" ~ "[1500, 2000)",
#                         Income=="Income 2000 - 2999" ~ "[2000, 3000)",
#                         Income=="Income 3000 - 3999" ~ "[3000, 4000)",
#                         Income=="Income 4000 - 4999" ~ "[4000, 5000)"))
```

## 1. Introduction
In this report, we aim to exploit both RFM modeling and MBA, using transaction data collected over the past 3 years, to help guide the company to make strategic business decisions to improve profitability.

We are provided with 3 datasets, namely `DSA3101_Hackathon_Categories_Information.csv`,
`DSA3101_Hackathon_Data.csv` and `DSA3101_Hackathon_Panelists_Demographics.csv`.

## 2. Exploratory Data Analysis (EDA)
### 2.1. Preview of each dataset
#### `DSA3101_Hackathon_Categories_Information.csv` - 62 rows, 3 columns
The first 6 rows is as follows:
```{r echo=F}
cat_info %>% head() %>% kable()
# cat(unique(cat_info$Category), sep=", ")
```
The 62 categories are Baby Cereal, Beer, Belacan, Bird Nest, Biscuits, Bouilon, Butter, Cake, Canned Product, Cereal Beverage, Cereals, Cheese, Chicken Essence, Choc/Nut Spread, Chocolate, Coconut Milk, Coffee, Condensed/Evap Milk, Confectionery, Cooking Oils, Cooking Sauces, Cordials, Creamer, CSD, Cultured Milk, Drinking Water, Eggs, Energy Drinks, Flour, Frozen Food, Fruit/Veg Juices, Ghee, Honey, Ice Cream, Instant Noodles, Instant Soup, Isotonic Drinks, Jam, Kaya, Liquid Milk, Margarine, Milk Powder-Adult, Milk Powder-Infant, Milk Powder-Kids, MSG, Peanut Butter, Rice, RTD Coffee, RTD Tea, Salad Dressing, Savoury Spread, Seasoning Powder, Snack, Soy Milk, Spagetti, Spirits, Sugar, Tea, Tonic Food Drink, Wine, Yoghurt Drink, Yoghurts.

The summary statistics is shown below:
```{r echo=F}
select_if(cat_info, is.numeric) %>% summary() %>% kable()
```
Cross-referencing with other sources, we found that the zero calories (MSG and Drinking Water) is indeed true.

-------------------

#### `DSA3101_Hackathon_Data.csv` - 1318024 rows, 6 columns
The first 6 rows is as follows:
```{r echo=F}
data %>% head() %>% kable()
```
This dataset consists of 156 Sundays (3 full years) which spans from 2017-06-25 to 2020-06-14.

The summary statistics is shown below:
```{r echo=F}
select_if(data, is.numeric) %>% summary() %>% kable()
```
There are entries with value zero in the 3 numerical columns. 
It could possibly be due to rounding errors (as the values are rounded to nearest 1 decimal place) or
simply a mistake in recording.

-------------------

#### `DSA3101_Hackathon_Panelists_Demographics.csv` - 4026 rows, 8 columns
The first 6 rows is as follows:
```{r echo=F, message=F}
panel_demo %>% head() %>% kable()
```

```{r echo=F, message=F}
lapply(colnames(panel_demo)[2:8], 
       function(x){table(panel_demo[[x]])/nrow(panel_demo)}) %>% 
  kable(col.names=c("", "Proportion"))
```

### 2.2. Background Information 

```{r echo=F, message=F, eval=F}
panel_demo %>% 
  group_by(Race, Income) %>% summarise(n=n()) %>% 
  mutate(n=n/sum(n)) %>%
  spread(key=Income, value=n) %>% kable()
```

```{r echo=F, message=F}
panel_demo %>% 
  group_by(location, Income) %>% summarise(n=n()) %>%
  mutate(n=n/sum(n)) %>%
  spread(key=Income, value=n) %>% kable()
```
It is of no surprise that Central (Capital of Malaysia - Kuala Lumpur) and South (Closer to Singapore) regions are generally more wealthy than North and East Coast.

```{r echo=F, message=F, fig.width=10}
data %>% 
  filter(Category %in% 
           c("Rice", "Canned Product", "Biscuits", "Flour", "Frozen Food",
             "Sugar", "Instant Noodles")) %>%
  group_by(Date, Category) %>% summarise(n=n()) %>% ggplot() +
  geom_line(aes(x=Date, y=n, color=Category)) + 
  geom_vline(xintercept=as.Date("2020-03-22"), colour="black", linetype="dotted") +
  theme(legend.position = "bottom") + ylab("Number of Transactions")
```
Malaysia announced their Movement Control Order (MCO) on 2020-03-18 (Wednesday).
Essential goods like rice, frozen food spiked in the number of transactions for that week.


-------------------

## 3. Preprocessing

### 3.1 Data Imputation
From the summary statistics, we have identified that there was erroneous data for three different categories of **Pack Size, Volume and Spend**

Here are some examples below:

```{r echo = F}
head(filter(data, `Pack Size` == 0 | `Volume` == 0 | `Spend` == 0)) %>% kable()
```
We observe that there are 49539 such instances of misrecorded/missing data therefore, we propose a few ways to handle the missing data. 

1. Since the missing data takes up 3.8% of the entire dataset, we can choose to disregard the missing data as missing data that is less than 5% of the dataset is usually inconsequential. (Schafer, 1999)

2.We can choose to impute the data using kNN imputation. However, we will need to decide which data we want to use to impute. 

We will try to impute the data using the median, as we observe that the dataset has some great outliers. (Anil et al, 2019)

```{r echo=F, message=F, warning=F}
# import python packages here
np <- reticulate::import('numpy')
pd <- reticulate::import('pandas')
preprocessing <- reticulate::import('sklearn.preprocessing')
enable_iterative_imputer <- reticulate::import('sklearn.experimental.enable_iterative_imputer')
impute <- reticulate::import('sklearn.impute')
```

We first encode these different categories into labels.

```{python}
data_cleaned = r.data
data_cleaned['Date'] = r.pd.to_datetime(data_cleaned['Date'])
le = r.preprocessing.LabelEncoder()
categories = data_cleaned.loc[:,"Category"]
le.fit(categories)
newcat = le.transform(categories)
data_cleaned = data_cleaned.assign(Category = newcat)
```
Next, we replace all the zero values in these categories to NaN for imputation

```{python}
cols = ["Volume","Pack Size","Spend"]
data_cleaned[cols] = data_cleaned[cols].replace({0:r.np.nan})
```

Next, we do impute with the median value
```{python}
imp_median = r.impute.IterativeImputer(random_state=0,initial_strategy = "median")
imp_median.fit(data_cleaned[["Volume","Pack Size", "Spend"]])

Y = data_cleaned[["Volume","Pack Size", "Spend"]]
newY = imp_median.transform(Y)
```

Some final processing bits for the data
```{python}
subarray0 = r.np.delete(r.np.delete(newY,2, 1),1,1)
subarray1 = r.np.delete(r.np.delete(newY,0, 1),1,1)
subarray2 = r.np.delete(r.np.delete(newY,0, 1),0,1)
data_cleaned['Volume'] = r.np.round(subarray0,1)
data_cleaned['Pack Size'] = r.np.round(subarray1,0)
data_cleaned['Spend'] = r.np.round(subarray2,1)
data_cleaned['Category'] = categories
```

Testing to see if there are any errors with the new imputed data
```{python echo=F}
data = data_cleaned
```

```{r echo=F}
py$data_cleaned %>% select(4:6) %>% summary() %>% kable()
```

Examples from the previous dataset against the new imputed.

```{python echo=F}
filter_1 = data_cleaned['Panel ID'] == "Panel 108052110" 
example_cleaned = data_cleaned[filter_1]
```

```{r echo=F}
data %>% filter(Volume == 0, `Panel ID` == "Panel 108052110") %>% 
  head(2) %>% kable()
py$example_cleaned %>% mutate(Date=as_date(Date)) %>% head(2) %>% kable()
```

We can see that the difference in the imputation comparing the before and after has significant improvements in the data, and we are confident that it is an accurate guess of the missing data.


-------------------

## 4. Analysis

### 4.1. RFM Modelling over Time
Assumptions:

- Weekly data represents the purchases a customer has made in that particular week.
Since there is no data on receipts, we shall treat this as a single visit for computing frequency score.
- Example: Customer A who has made 10 transactions 4 weeks ago is not as frequent as 
Customer B who has made 1 transaction each consistently for 4 weeks.
- Approach: We group by the Panel ID and Date first to compile the transactions in 1 single receipt 
for that week. Then we group by Panel ID to apply RFM. The monetary is computed based on mean expenditure
rather than the total sum to as the total sum is easily influenced by the frequency of the visits
(collinearity issues)

Due to the largely skewed dataset, we will not be performing quantile cuts.
Instead, we will be doing manual segmentation as it is more robust and the results are more interpretable.

- Recency
  - 3: Last visit was <= 1 week ago
  - 2: Last visit was  <= 2 weeks ago
  - 1: Last visit was > 2 weeks ago
- Frequency
  - 3: >= 9 visits in 12 weeks
  - 2: < 9 visits in 12 weeks (average twice every three weeks)
  - 1: < 6 visits in 12 weeks (average fortnightly visits)
- Monetary
  - 3: >= RM60 mean spending in a week
  - 2: < RM60 mean spending in a week
  - 1: < RM30 mean spending in a week

```{r echo=F, message=F, fig.width=10, fig.height=8}
maxDate <- max(data$Date)
mcoDate <- ymd("2020-03-18")
mcoPrevSunDate <- ymd("2020-03-15")
minDate <- min(data$Date)

postMCO_weeks <- as.numeric(difftime(maxDate, mcoPrevSunDate, units="weeks"))

threeMonths <- data.frame()
for (i in 1:12){
  x <- data %>% 
    filter((Date < mcoDate - postMCO_weeks*7*(11-i)) & 
             (Date > mcoDate - postMCO_weeks*7*(12-i))) %>%
    group_by(`Panel ID`, `Date`) %>% summarise(Total=sum(Spend)) %>% 
    group_by(`Panel ID`) %>% 
    summarise(Recency = as.numeric(mcoPrevSunDate - postMCO_weeks*7*(11-i) - max(Date))/7,    # convert to in terms of weeks
              Frequency = n(),    
              Monetary = mean(Total)) %>% 
    mutate(Recency = case_when(Recency == 0 ~ 3,    # difference of weeks starts from 1
                               Recency == 1 ~ 2,
                               TRUE ~ 1),
           Frequency = case_when(Frequency < 6 ~ 1,    # rarer than fortnightly visits
                               Frequency < 9 ~ 2,    # 2-3 average weekly visits per month
                               TRUE ~ 3),    # 3-4 average weekly visits per month
           Monetary = case_when(Monetary < 40 ~ 1,    # less than RM40/month
                               Monetary < 60 ~ 2,    # less than RM60/month
                               TRUE ~ 3),    # >= RM60/month
           RFM = str_c(Recency, Frequency, Monetary), Window=i)
  threeMonths <- rbind(threeMonths, x)
}

threeMonths_df <- threeMonths %>% group_by(Window, RFM) %>% summarise(n=n()) %>% 
  group_by(Window) %>% summarise(RFM=RFM, Proportion=n/sum(n))
threeMonths_33x <- threeMonths_df %>% filter(Proportion > 0.1)
threeMonths_31x <- threeMonths_df %>% 
  filter(Proportion < 0.1, str_sub(RFM, 1, 2) == "31")
threeMonths_13x <- threeMonths_df %>% 
  filter(Proportion < 0.1, str_sub(RFM, 1, 2) == "13")
threeMonths_11x <- threeMonths_df %>% 
  filter(Proportion < 0.1, str_sub(RFM, 1, 2) == "11")
threeMonths_others <- threeMonths_df %>% 
  filter(Proportion < 0.1)

ggplot() + 
  geom_line(aes(x=Window, y=Proportion, group=RFM, 
                color="Others"), 
            data=threeMonths_others, size=1.01) +
  geom_line(aes(x=Window, y=Proportion, color=RFM), 
            data=threeMonths_33x, size=1.01) +
  geom_line(aes(x=Window, y=Proportion, group=RFM, 
                color="New: {311, 312, 313}"), 
            data=threeMonths_31x, size=1.01) + 
  geom_line(aes(x=Window, y=Proportion, group=RFM, 
                color="Churn risk: {131, 132, 133}"), 
            data=threeMonths_13x, size=1.01) +
  geom_line(aes(x=Window, y=Proportion, group=RFM, 
                color="Lost: {111, 112, 113}"), 
            data=threeMonths_11x, size=1.01) +
  scale_x_continuous(breaks=seq(1,12)) + theme_bw() +
  theme(legend.position = "bottom", panel.border = element_blank()) + 
  scale_color_brewer(palette="Dark2")

```
In general, there are no big changes (with the exception of `RFM = 331`) in terms of the proportion of the RFM for in each window (13 weeks period). 
It is important for us to ensure that the customers who are at risk of churning remains low.
From our segmentation, we can see that most of our customers (~50%) are loyal patrons who visit frequently (score 3) and recently (score 3). 
The outbreak of Coronavirus occurred some time in between window 10 and 11 while 
the MCO happened just before window 12. 
It is obvious that many customers are spending more during the pandemic as seen from the spike in `RFM = 333`.

#### Pre MCO VS Post MCO Period
```{r echo=F, message=F}
preMCO <- data %>% filter((Date < mcoDate) & (Date > mcoDate - postMCO_weeks*7)) %>%
  group_by(`Panel ID`, `Date`) %>% summarise(Total=sum(Spend)) %>% 
  group_by(`Panel ID`) %>% 
  summarise(Recency = as.numeric(mcoPrevSunDate - max(Date))/7,    # convert to in terms of weeks
            Frequency = n(),    
            Monetary = mean(Total)) %>% 
  mutate(Recency = case_when(Recency == 0 ~ 3,    # difference of weeks starts from 1
                             Recency == 1 ~ 2,
                             TRUE ~ 1),
         Frequency = case_when(Frequency < 6 ~ 1,    # rarer than fortnightly visits
                             Frequency < 9 ~ 2,    # 2-3 average weekly visits per month
                             TRUE ~ 3),    # 3-4 average weekly visits per month
         Monetary = case_when(Monetary < 40 ~ 1,    # less than RM40/month
                             Monetary < 60 ~ 2,    # less than RM60/month
                             TRUE ~ 3),    # >= RM60/month
         event = "pre", RFM = str_c(Recency, Frequency, Monetary)) %>% 
  left_join(panel_demo, by=c("Panel ID"="ID"))

postMCO <- data %>% filter(Date > mcoDate) %>%
  group_by(`Panel ID`, `Date`) %>% summarise(Total=sum(Spend)) %>% 
  group_by(`Panel ID`) %>% 
  summarise(Recency = as.numeric(maxDate - max(Date))/7,    # convert to in terms of weeks
            Frequency = n(),    
            Monetary = mean(Total)) %>% 
  mutate(Recency = case_when(Recency == 0 ~ 3,    # difference of weeks starts from 0
                             Recency == 1 ~ 2,
                             TRUE ~ 1),
         Frequency = case_when(Frequency < 6 ~ 1,    # rarer than fortnightly visits
                             Frequency < 9 ~ 2,    # 2-3 average weekly visits per month
                             TRUE ~ 3),    # 3-4 average weekly visits per month
         Monetary = case_when(Monetary < 40 ~ 1,    # less than RM40/month
                             Monetary < 60 ~ 2,    # less than RM60/month
                             TRUE ~ 3),    # >= RM60/month
         event = "post", RFM = str_c(Recency, Frequency, Monetary)) %>% 
  left_join(panel_demo, by=c("Panel ID"="ID"))

postMCOHIHM <- filter(postMCO, Income == "[5000, )", Monetary == 3)$`Panel ID`
postMCOHILM <- filter(postMCO, Income == "[5000, )", Monetary == 1)$`Panel ID`
# data %>% filter(Date > mcoDate, `Panel ID` %in% postMCOHIHM) %>%
#   write_csv("../cleaned_data/postMCOHIHM.csv")
# data %>% filter(Date > mcoDate, `Panel ID` %in% postMCOHILM) %>%
#   write_csv("../cleaned_data/postMCOHILM.csv")
preMCOHIHM <- filter(preMCO, Income == "[5000, )", Monetary == 3)$`Panel ID`
preMCOHILM <- filter(preMCO, Income == "[5000, )", Monetary == 1)$`Panel ID`
# data %>% filter((Date < mcoDate) & (Date > mcoDate - postMCO_weeks*7), 
#                 `Panel ID` %in% preMCOHIHM) %>% 
#   write_csv("../cleaned_data/preMCOHIHM.csv")
# data %>% filter((Date < mcoDate) & (Date > mcoDate - postMCO_weeks*7), 
#                 `Panel ID` %in% preMCOHILM) %>% 
#   write_csv("../cleaned_data/preMCOHILM.csv")

stayed <- intersect(postMCO$`Panel ID`, preMCO$`Panel ID`)
relative <- rbind(preMCO, postMCO) %>% filter(`Panel ID` %in% stayed)
relative %>% group_by(`Panel ID`) %>% 
  summarise(Change=if_else(first(RFM)==last(RFM), "No Change", 
                           str_c(first(RFM), " -> ", last(RFM)))) %>% 
  group_by(Change) %>% summarise(n=n()) %>% mutate(Proportion=n/sum(n)) %>% 
  arrange(desc(Proportion)) %>% head() %>% kable()
```
Among the customers who had stayed on, most of them have no changes in RFM scores.
We see a significant increase in terms of Monetary score so it would be wise
for us to try to retain as many of them as possible.

#### Demographic Analysis
A closer examination of the Income subgroups of `RFM = 331` before MCO starts is shown as follows:
```{r echo=F, message=F}
preMCO %>% filter(Recency == 3, Frequency == 3, Monetary == 1) %>% 
  group_by(Income) %>% summarise(Counts=n()) %>% 
  mutate(Proportion=Counts/sum(Counts)) %>% kable()

```
In particular, one group of customers that are of interest are our loyal customers (**high R and F**) who are in the **high income bracket** but have **low M values**. Interestingly, this group of customer actually comprises a significant proportion among the high R and F segment! This further reinforces our beliefs to target them as they are capable of spending more. If we can entice these customers (RFM = 331) to change their consumption patterns to become more like their high income high M counterparts, it will help to increase revenue for the client!

```{r echo=F, message=F, eval=F}
high_income <- filter(panel_demo, Income == "[5000, )")
lapply(colnames(high_income)[c(2, 5:9)], 
       function(x){
         table(high_income[[x]])/nrow(high_income)}) %>% 
  kable(col.names=c("", "Proportion"))
```
The above shows the subgroups of Income > 5000 customers.

```{r echo=F, message=F}
demo_als <- preMCO %>% filter(Recency == 3, Frequency == 3, Monetary == 1, 
                  Income == "[5000, )")
lapply(colnames(preMCO)[c(7, 10:14)], 
       function(x){table(demo_als[[x]])/nrow(demo_als)}) %>% 
  kable(col.names=c("", "Proportion"))
```
The above shows the subgroups of Income > 5000 customers with `RFM = 331` before MCO.

```{r echo=F, message=F}
demo_als <- postMCO %>% filter(Recency == 3, Frequency == 3, Monetary == 1, 
                  Income == "[5000, )")
lapply(colnames(preMCO)[c(7, 10:14)], 
       function(x){table(demo_als[[x]])/nrow(demo_als)}) %>% 
  kable(col.names=c("", "Proportion"))
```
The above shows the subgroups of Income > 5000 customers with `RFM = 331` after MCO.

From the above analyses on the subgroups of customers in the high income bracket, 
they are largely coming from smaller households, 
which could partly explain the lower consumption (hence lower expenditure).


```{r echo=F, message=F}
m_inc <- relative %>% filter(Recency == 3, Frequency == 3) %>% 
  arrange(desc(event)) %>% group_by(`Panel ID`) %>% 
  summarise(IncreaseM=as.numeric(last(Monetary) > first(Monetary))) %>% 
  filter(IncreaseM == 1) %>% left_join(panel_demo, by=c("Panel ID"="ID"))

lapply(colnames(m_inc)[c(3,4,6:10)], 
       function(x){table(m_inc[[x]])/nrow(m_inc)}) %>% 
  kable(col.names=c("", "Proportion"))
```
The above shows the subgroups of customers with R = 3, F = 3 and experienced an increase in M after MCO.

-------------------

### 4.2. Market Basket Analysis

We thus now turn to analysing the market baskets of the 2 groups to explain the discrepancy and hopefully obtain some insight as to how we can promote greater spending.

Some findings in certain products much more common in high spending groups/certain luxury products? Whatever we manage to find (hopefully)

Based on our market basket analysis, focusing on these products, we have found that amongst the things high spenders buy more than low spenders, these things have the highest confidence:

- X->Y
- Y->Z
- X->Z
- Z->A
- Z->B

-------------------

## 5. Conclusion
Based on our analysis, we would thus suggest **innovative ways to increase sales based on MBA trying to promote item X/Z to low spending high income customers**

- Send the customers whose RFM score changes from either 331 to 332 or 331 to 333 or 332 to 333
a thank you note for being a good customer and give them a discount voucher to reward and
encourage their good customer behaviour. 
**The discount voucher can be applied on any item or of certain items depending on MBA analysis. Maybe the 333 customers can get a universal item discount voucher?**


## 6. Biblography

Schafer, J. L. (1999). Multiple imputation: a primer. Statistical Methods in Medical Research, 8(1), 3â€“15. https://doi.org/10.1177/096228029900800102

Anil Jadhav, Dhanya Pramod & Krishnan Ramanathan (2019) Comparison of Performance of Data Imputation Methods for Numeric Dataset, Applied Artificial Intelligence, 33:10, 913-933, DOI: 10.1080/08839514.2019.1637138



